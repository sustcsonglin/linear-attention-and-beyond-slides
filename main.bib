
@inproceedings{hgrn2,
  title={HGRN2: Gated Linear RNNs with State Expansion},
  author={Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269043328}
}
@article{Joffrain2006AccumulatingHT,
  title={Accumulating Householder transformations, revisited},
  author={Thierry Joffrain and Tze Meng Low and Enrique S. Quintana-Ort{\'i} and Robert A. van de Geijn and Field G. Van Zee},
  journal={ACM Trans. Math. Softw.},
  year={2006},
  volume={32},
  pages={169-179},
  url={https://api.semanticscholar.org/CorpusID:15723171}
}

@article{gla,
  author       = {Songlin Yang and
                  Bailin Wang and
                  Yikang Shen and
                  Rameswar Panda and
                  Yoon Kim},
  title        = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  journal      = {CoRR},
  volume       = {abs/2312.06635},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.06635},
  doi          = {10.48550/ARXIV.2312.06635},
  eprinttype    = {arXiv},
  eprint       = {2312.06635},
  timestamp    = {Thu, 04 Jan 2024 15:12:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-06635.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{siems2025deltaproductincreasingexpressivitydeltanet,
      title={DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders}, 
      author={Julien Siems and Timur Carstensen and Arber Zela and Frank Hutter and Massimiliano Pontil and Riccardo Grazzi},
      year={2025},
      eprint={2502.10297},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.10297}, 
}
@inproceedings{
sun2024you,
title={You Only Cache Once: Decoder-Decoder Architectures for Language Models},
author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=25Ioxw576r}
}
@article{Bai2023LongBenchAB,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Yushi Bai and Xin Lv and Jiajie Zhang and Hong Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.14508},
  url={https://api.semanticscholar.org/CorpusID:261245264}
}
@article{Hsieh2024RULERWT,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.06654},
  url={https://api.semanticscholar.org/CorpusID:269032933}
}
@misc{minimax2025minimax01scalingfoundationmodels,
      title={MiniMax-01: Scaling Foundation Models with Lightning Attention}, 
      author={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},
      year={2025},
      eprint={2501.08313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.08313}, 
}
@article{Qin2024VariousLC,
  title={Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention},
  author={Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.17381},
  url={https://api.semanticscholar.org/CorpusID:270063820}
}
@inproceedings{Munkhdalai2024LeaveNC,
  title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author={Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269033427}
}
@article{DBLP:journals/corr/abs-2402-18510,
  author       = {Kaiyue Wen and
                  Xingyu Dang and
                  Kaifeng Lyu},
  title        = {RNNs are not Transformers (Yet): The Key Bottleneck on In-context
                  Retrieval},
  journal      = {CoRR},
  volume       = {abs/2402.18510},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.18510},
  doi          = {10.48550/ARXIV.2402.18510},
  eprinttype    = {arXiv},
  eprint       = {2402.18510},
  timestamp    = {Tue, 26 Mar 2024 10:51:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-18510.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{repeat_after_me,
  author       = {Samy Jelassi and
                  David Brandfonbrener and
                  Sham M. Kakade and
                  Eran Malach},
  title        = {Repeat After Me: Transformers are Better than State Space Models at
                  Copying},
  journal      = {CoRR},
  volume       = {abs/2402.01032},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.01032},
  doi          = {10.48550/ARXIV.2402.01032},
  eprinttype    = {arXiv},
  eprint       = {2402.01032},
  timestamp    = {Fri, 09 Feb 2024 12:18:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-01032.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Widrow1988AdaptiveSC,
  title={Adaptive switching circuits},
  author={Bernard Widrow and Marcian E. Hoff},
  year={1988},
  url={https://api.semanticscholar.org/CorpusID:60830585}
}

@inproceedings{wy,
  title={The WY representation for products of householder matrices},
  author={Christian H. Bischof and Charles Van Loan},
  booktitle={SIAM Conference on Parallel Processing for Scientific Computing},
  year={1985},
  url={https://api.semanticscholar.org/CorpusID:36094006}
}

@inproceedings{Munkhdalai2024LeaveNC,
  title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author={Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269033427}
}

@article{based,
  author       = {Simran Arora and
                  Sabri Eyuboglu and
                  Michael Zhang and
                  Aman Timalsina and
                  Silas Alberti and
                  Dylan Zinsley and
                  James Zou and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Simple linear attention language models balance the recall-throughput
                  tradeoff},
  journal      = {CoRR},
  volume       = {abs/2402.18668},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.18668},
  doi          = {10.48550/ARXIV.2402.18668},
  eprinttype    = {arXiv},
  eprint       = {2402.18668},
  timestamp    = {Tue, 26 Mar 2024 10:51:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-18668.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peng2024EagleAF,
  title={Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence},
  author={Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Teddy Ferdinan and Haowen Hou and Przemys l aw Kazienko and G Kranthikiran and Jan Koco'n and Bartlomiej Koptyra and Satyapriya Krishna and Ronald McClelland and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Stanislaw Wo'zniak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269010053}
}

@article{yang2024vivim,
  title={Vivim: a Video Vision Mamba for Medical Video Object Segmentation},
  author={Yang, Yijun and Xing, Zhaohu and Zhu, Lei},
  journal={arXiv preprint arXiv:2401.14168},
  year={2024}
}

@article{ma2024u,
  title={U-mamba: Enhancing long-range dependency for biomedical image segmentation},
  author={Ma, Jun and Li, Feifei and Wang, Bo},
  journal={arXiv preprint arXiv:2401.04722},
  year={2024}
}
@article{zhu2024vision,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}
@article{liu2024vmamba,
  title={Vmamba: Visual state space model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}
@article{xing2024segmamba,
  title={Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation},
  author={Xing, Zhaohu and Ye, Tian and Yang, Yijun and Liu, Guang and Zhu, Lei},
  journal={arXiv preprint arXiv:2401.13560},
  year={2024}
}

@article{wang2024mambabyte,
  title={MambaByte: Token-free Selective State Space Model},
  author={Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
  journal={arXiv preprint arXiv:2401.13660},
  year={2024}
}
@article{wang2024graph,
  title={Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces},
  author={Wang, Chloe and Tsepa, Oleksii and Ma, Jun and Wang, Bo},
  journal={arXiv preprint arXiv:2402.00789},
  year={2024}
}

@article{Oren2024TransformersAM,
  title={Transformers are Multi-State RNNs},
  author={Matanel Oren and Michael Hassid and Yossi Adi and Roy Schwartz},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.06104},
}

@article{DBLP:journals/corr/abs-2310-20051,
  author       = {Zhao Song and
                  Guangyi Xu and
                  Junze Yin},
  title        = {The Expressibility of Polynomial based Attention Scheme},
  journal      = {CoRR},
  volume       = {abs/2310.20051},
  year         = {2023},
  doi          = {10.48550/ARXIV.2310.20051},
  eprinttype    = {arXiv},
  eprint       = {2310.20051},
  timestamp    = {Fri, 03 Nov 2023 10:56:40 +0100},
}

@inproceedings{cosformer,
  author       = {Zhen Qin and
                  Weixuan Sun and
                  Hui Deng and
                  Dongxu Li and
                  Yunshen Wei and
                  Baohong Lv and
                  Junjie Yan and
                  Lingpeng Kong and
                  Yiran Zhong},
  title        = {cosFormer: Rethinking Softmax In Attention},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},  
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},  
}

@misc{Hedgehog,
Author = {Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Ré},
Title = {The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
Year = {2024},
Eprint = {arXiv:2402.04347},
}
@misc{log-normal,
Author = {Yury Nahshan and Joseph Kampeas and Emir Haleva},
Title = {Linear Log-Normal Attention with Unbiased Concentration},
Year = {2023},
Eprint = {arXiv:2311.13541},
}

@inproceedings{performer,
  author       = {Krzysztof Marcin Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  Xingyou Song and
                  Andreea Gane and
                  Tam{\'{a}}s Sarl{\'{o}}s and
                  Peter Hawkins and
                  Jared Quincy Davis and
                  Afroz Mohiuddin and
                  Lukasz Kaiser and
                  David Benjamin Belanger and
                  Lucy J. Colwell and
                  Adrian Weller},
  title        = {Rethinking Attention with Performers},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@article{Alman2023FastAR,
  title={Fast Attention Requires Bounded Entries},
  author={Josh Alman and Zhao Song},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13214}
}

@misc{orthogonal_memory,
Author = {Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
Title = {Linear Attention via Orthogonal Memory},
Year = {2023},
Eprint = {arXiv:2312.11135},
}

@inproceedings{zhang-cai-2022-linearizing,
    title = "Linearizing Transformer with Key-Value Memory",
    author = "Zhang, Yizhe  and
      Cai, Deng",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{peng-etal-2022-abc,
    title = "{ABC}: Attention with Bounded-memory Control",
    author = "Peng, Hao  and
      Kasai, Jungo  and
      Pappas, Nikolaos  and
      Yogatama, Dani  and
      Wu, Zhaofeng  and
      Kong, Lingpeng  and
      Schwartz, Roy  and
      Smith, Noah A.",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
}


@misc{polysketch,
Author = {Praneeth Kacham and Vahab Mirrokni and Peilin Zhong},
Title = {PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels},
Year = {2023},
Eprint = {arXiv:2310.01655},
}

@inproceedings{hyena,
  author       = {Michael Poli and
                  Stefano Massaroli and
                  Eric Nguyen and
                  Daniel Y. Fu and
                  Tri Dao and
                  Stephen Baccus and
                  Yoshua Bengio and
                  Stefano Ermon and
                  Christopher R{\'{e}}},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {28043--28078},
  publisher    = {{PMLR}},
  year         = {2023},
}
@inproceedings{LRU,
  author       = {Antonio Orvieto and
                  Samuel L. Smith and
                  Albert Gu and
                  Anushan Fernando and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Razvan Pascanu and
                  Soham De},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Resurrecting Recurrent Neural Networks for Long Sequences},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {26670--26698},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/orvieto23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/OrvietoSGFGPD23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mega,
  author       = {Xuezhe Ma and
                  Chunting Zhou and
                  Xiang Kong and
                  Junxian He and
                  Liangke Gui and
                  Graham Neubig and
                  Jonathan May and
                  Luke Zettlemoyer},
  title        = {Mega: Moving Average Equipped Gated Attention},
  journal      = {CoRR},
  volume       = {abs/2209.10655},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2209.10655},
  doi          = {10.48550/arXiv.2209.10655},
  eprinttype    = {arXiv},
  eprint       = {2209.10655},
  timestamp    = {Wed, 28 Sep 2022 15:17:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2209-10655.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rwkv6,
  title={Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence},
  author={Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Teddy Ferdinan and Haowen Hou and Przemys l aw Kazienko and G Kranthikiran and Jan Koco'n and Bartlomiej Koptyra and Satyapriya Krishna and Ronald McClelland and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Stanislaw Wo'zniak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
  year={2024}
}
@article{GRU,
  author       = {Junyoung Chung and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  KyungHyun Cho and
                  Yoshua Bengio},
  title        = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
                  Modeling},
  journal      = {CoRR},
  volume       = {abs/1412.3555},
  year         = {2014},
  url          = {http://arxiv.org/abs/1412.3555},
  eprinttype    = {arXiv},
  eprint       = {1412.3555},
  timestamp    = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Griffin,
  title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando de Freitas and Caglar Gulcehre},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.19427},
  url={https://api.semanticscholar.org/CorpusID:268091246}
}

@inproceedings{lightning2,
  title={Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
  author={Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
  year={2024},
}
@inproceedings{h3,
  author       = {Daniel Y. Fu and
                  Tri Dao and
                  Khaled Kamal Saab and
                  Armin W. Thomas and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
}

@article{zoology,
  author       = {Simran Arora and
                  Sabri Eyuboglu and
                  Aman Timalsina and
                  Isys Johnson and
                  Michael Poli and
                  James Zou and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Zoology: Measuring and Improving Recall in Efficient Language Models},
  journal      = {CoRR},
  volume       = {abs/2312.04927},
  year         = {2023},
}
@inproceedings{Blelloch1990PrefixSA,
  title={Prefix sums and their applications},
  author={Guy E. Blelloch},
  year={1990},
}
@inproceedings{s5,
  author       = {Jimmy T. H. Smith and
                  Andrew Warrington and
                  Scott W. Linderman},
  title        = {Simplified State Space Layers for Sequence Modeling},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
}

@inproceedings{s4,
  author       = {Albert Gu and
                  Karan Goel and
                  Christopher R{\'{e}}},
  title        = {Efficiently Modeling Long Sequences with Structured State Spaces},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
}

@article{recurrent_linear_xfmr,
  author       = {Subhojeet Pramanik and
                  Esraa Elelimy and
                  Marlos C. Machado and
                  Adam White},
  title        = {Recurrent Linear Transformers},
  journal      = {CoRR},
  volume       = {abs/2310.15719},
  year         = {2023},
}

@article{DBLP:journals/neco/GersSC00,
  author       = {Felix A. Gers and
                  J{\"{u}}rgen Schmidhuber and
                  Fred A. Cummins},
  title        = {Learning to Forget: Continual Prediction with {LSTM}},
  journal      = {Neural Comput.},
  volume       = {12},
  number       = {10},
  pages        = {2451--2471},
  year         = {2000},
}
@article{Saphra2023FirstTT,
  title={First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models},
  author={Naomi Saphra and Eve Fleisig and Kyunghyun Cho and Adam Lopez},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.05020},
}
@article{Li2023LightSeqSL,
  title={LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers},
  author={Dacheng Li and Rulin Shao and Anze Xie and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Xuezhe Ma and Hao Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03294},
}
@inproceedings{Yan2023DiffusionMW,
  title={Diffusion Models Without Attention},
  author={Jing Nathan Yan and Jiatao Gu and Alexander M. Rush},
  year={2023},
}
@article{Brandon2023StripedAF,
  title={Striped Attention: Faster Ring Attention for Causal Transformers},
  author={William Brandon and Aniruddha Nrusimha and Kevin Qian and Zachary Ankner and Tian Jin and Zhiye Song and Jonathan Ragan-Kelley},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.09431},
}

@article{Liu2023RingAW,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Hao Liu and Matei Zaharia and Pieter Abbeel},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01889},
}
@inproceedings{Chaurasia2015CompilingHP,
  title={Compiling high performance recursive filters},
  author={Gaurav Chaurasia and Jonathan Ragan-Kelley and Sylvain Paris and George Drettakis and Fr{\'e}do Durand},
  booktitle={High Performance Graphics},
  year={2015},
}
@article{Fu2023MonarchMA,
  title={Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture},
  author={Daniel Y. Fu and Simran Arora and Jessica Grogan and Isys Johnson and Sabri Eyuboglu and Armin W. Thomas and Benjamin Spector and Michael Poli and Atri Rudra and Christopher R'e},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.12109},
}
@inproceedings{Dao2022MonarchES,
  title={Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  author={Tri Dao and Beidi Chen and Nimit Sharad Sohoni and Arjun D Desai and Michael Poli and Jessica Grogan and Alexander Liu and Aniruddh Rao and Atri Rudra and Christopher R{\'e}},
  booktitle={International Conference on Machine Learning},
  year={2022},
}

@article{Hooker2020TheHL,
  title={The hardware lottery},
  author={Sara Hooker},
  journal={Communications of the ACM},
  year={2020},
  volume={64},
  pages={58 - 65},
}

@inproceedings{li-etal-2023-sequence,
    title = "Sequence Parallelism: Long Sequence Training from System Perspective",
    author = "Li, Shenggui  and
      Xue, Fuzhao  and
      Baranwal, Chaitanya  and
      Li, Yongbin  and
      You, Yang",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
}


@inproceedings{Gu2023MambaLS,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Albert Gu and Tri Dao},
  year={2023},
}
@INPROCEEDINGS{9365803,
  author={Choquette, Jack and Lee, Edward and Krashinsky, Ronny and Balan, Vishnu and Khailany, Brucek},
  booktitle={2021 IEEE International Solid-State Circuits Conference (ISSCC)}, 
  title={3.2 The A100 Datacenter GPU and Ampere Architecture}, 
  year={2021},
  volume={64},
  number={},
  pages={48-50},
  doi={10.1109/ISSCC42613.2021.9365803}}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optdoi      = {10.1162/neco.1997.9.8.1735},
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{tcfft,
  author       = {Bin{-}Rui Li and
                  Shenggan Cheng and
                  James Lin},
  title        = {tcFFT: Accelerating Half-Precision {FFT} through Tensor Cores},
  journal      = {CoRR},
  volume       = {abs/2104.11471},
  year         = {2021},
}
@article{flashconvfft,
  author       = {Daniel Y. Fu and
                  Hermann Kumbong and
                  Eric Nguyen and
                  Christopher R{\'{e}}},
  title        = {FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor
                  Cores},
  journal      = {CoRR},
  volume       = {abs/2311.05908},
  year         = {2023},
}
@inproceedings{DBLP:conf/ipps/PishaL21,
  author       = {Louis Pisha and
                  Lukasz Ligowski},
  title        = {Accelerating non-power-of-2 size Fourier transforms with {GPU} Tensor
                  Cores},
  booktitle    = {35th {IEEE} International Parallel and Distributed Processing Symposium,
                  {IPDPS} 2021, Portland, OR, USA, May 17-21, 2021},
  pages        = {507--516},
  publisher    = {{IEEE}},
  year         = {2021},
}
@inproceedings{DBLP:conf/ics/DakkakLXGH19,
  author       = {Abdul Dakkak and
                  Cheng Li and
                  Jinjun Xiong and
                  Isaac Gelado and
                  Wen{-}Mei W. Hwu},
  editor       = {Rudolf Eigenmann and
                  Chen Ding and
                  Sally A. McKee},
  title        = {Accelerating reduction and scan using tensor core units},
  booktitle    = {Proceedings of the {ACM} International Conference on Supercomputing,
                  {ICS} 2019, Phoenix, AZ, USA, June 26-28, 2019},
  pages        = {46--57},
  publisher    = {{ACM}},
  year         = {2019},
}
@article{pretrain_wo_attn,
  author       = {Junxiong Wang and
                  Jing Nathan Yan and
                  Albert Gu and
                  Alexander M. Rush},
  title        = {Pretraining Without Attention},
  journal      = {CoRR},
  volume       = {abs/2212.10544},
  year         = {2022},
}
@inproceedings{gss,
  author       = {Harsh Mehta and
                  Ankit Gupta and
                  Ashok Cutkosky and
                  Behnam Neyshabur},
  title        = {Long Range Language Modeling via Gated State Spaces},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
}
@article{swish,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017},
}
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{rope,
  author       = {Jianlin Su and
                  Yu Lu and
                  Shengfeng Pan and
                  Bo Wen and
                  Yunfeng Liu},
  title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal      = {CoRR},
  volume       = {abs/2104.09864},
  year         = {2021},
  eprinttype    = {arXiv},
  eprint       = {2104.09864},
}

@inproceedings{ft_xfmr_into_rnn,
  author       = {Jungo Kasai and
                  Hao Peng and
                  Yizhe Zhang and
                  Dani Yogatama and
                  Gabriel Ilharco and
                  Nikolaos Pappas and
                  Yi Mao and
                  Weizhu Chen and
                  Noah A. Smith},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {Finetuning Pretrained Transformers into RNNs},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
                  Republic, 7-11 November, 2021},
  pages        = {10630--10643},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  doi          = {10.18653/V1/2021.EMNLP-MAIN.830},
}
@article{flashattention2,
  author       = {Tri Dao},
  title        = {FlashAttention-2: Faster Attention with Better Parallelism and Work
                  Partitioning},
  journal      = {CoRR},
  volume       = {abs/2307.08691},
  year         = {2023},
  doi          = {10.48550/ARXIV.2307.08691},
  eprinttype    = {arXiv},
  eprint       = {2307.08691},
}
@inproceedings{flashattention1,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {NeurIPS},
  year         = {2022},
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@article{qin2023scaling,
  title={Scaling transnormer to 175 billion parameters},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others},
  journal={arXiv preprint arXiv:2307.14995},
  year={2023}
}

@misc{gu2022efficiently,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rwkv,
  author       = {Bo Peng and
                  Eric Alcaide and
                  Quentin Anthony and
                  Alon Albalak and
                  Samuel Arcadinho and
                  Huanqi Cao and
                  Xin Cheng and
                  Michael Chung and
                  Matteo Grella and
                  Kranthi Kiran G. V. and
                  Xuzheng He and
                  Haowen Hou and
                  Przemyslaw Kazienko and
                  Jan Kocon and
                  Jiaming Kong and
                  Bartlomiej Koptyra and
                  Hayden Lau and
                  Krishna Sri Ipsit Mantri and
                  Ferdinand Mom and
                  Atsushi Saito and
                  Xiangru Tang and
                  Bolun Wang and
                  Johan S. Wind and
                  Stanislaw Wozniak and
                  Ruichong Zhang and
                  Zhenyuan Zhang and
                  Qihang Zhao and
                  Peng Zhou and
                  Jian Zhu and
                  Rui{-}Jie Zhu},
  title        = {{RWKV:} Reinventing RNNs for the Transformer Era},
  journal      = {CoRR},
  volume       = {abs/2305.13048},
  year         = {2023},
  doi          = {10.48550/ARXIV.2305.13048},
  eprinttype    = {arXiv},
  eprint       = {2305.13048},
}

@article{HGRN,
  author       = {Zhen Qin and
                  Songlin Yang and
                  Yiran Zhong},
  title        = {Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2311.04823},
  year         = {2023},
  doi          = {10.48550/ARXIV.2311.04823},
  eprinttype    = {arXiv},
  eprint       = {2311.04823},
  timestamp    = {Tue, 14 Nov 2023 14:47:55 +0100},
}

@article{VQ-Transformer,
  author       = {Lucas D. Lingle},
  title        = {Transformer-VQ: Linear-Time Transformers via Vector Quantization},
  journal      = {CoRR},
  volume       = {abs/2309.16354},
  year         = {2023},
  doi          = {10.48550/ARXIV.2309.16354},
  eprinttype    = {arXiv},
  eprint       = {2309.16354},
}

@inproceedings{GAU,
  author       = {Weizhe Hua and
                  Zihang Dai and
                  Hanxiao Liu and
                  Quoc V. Le},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {Transformer Quality in Linear Time},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {9099--9117},
  publisher    = {{PMLR}},
  year         = {2022},
}

@inproceedings{
yang2024parallelizing,
title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y8Rm4VNRPH}
}
@article{Mereghetti2000ThresholdCF,
  title={Threshold circuits for iterated matrix product and powering},
  author={Carlo Mereghetti and Beatrice Palano},
  journal={RAIRO Theor. Informatics Appl.},
  year={2000},
  volume={34},
  pages={39-46},
  url={https://api.semanticscholar.org/CorpusID:13237763}
}
@article{Liu2024LonghornSS,
  title={Longhorn: State Space Models are Amortized Online Learners},
  author={Bo Liu and Rui Wang and Lemeng Wu and Yihao Feng and Peter Stone and Qian Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.14207},
  url={https://api.semanticscholar.org/CorpusID:271310065}
}
@article{Sun2024YouOC,
  title={You Only Cache Once: Decoder-Decoder Architectures for Language Models},
  author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.05254},
  url={https://api.semanticscholar.org/CorpusID:269626143}
}
@misc{behrouz2024titanslearningmemorizetest,
      title={Titans: Learning to Memorize at Test Time}, 
      author={Ali Behrouz and Peilin Zhong and Vahab Mirrokni},
      year={2024},
      eprint={2501.00663},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.00663}, 
}
@article{sun2024learning,
  title={Learning to (Learn at Test Time): RNNs with Expressive Hidden States},
  author={Yu Sun and Xinhao Li and Karan Dalal and Jiarui Xu and Arjun Vikram and Genghan Zhang and Yann Dubois and Xinlei Chen and Xiaolong Wang and Oluwasanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.04620},
  url={https://api.semanticscholar.org/CorpusID:271039606}
}

@inproceedings{
jelassi2024repeat,
title={Repeat After Me: Transformers are Better than State Space Models at Copying},
author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and eran malach},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=duRRoGeoQT}
}
@inproceedings{
beck2024xlstm,
title={x{LSTM}: Extended Long Short-Term Memory},
author={Maximilian Beck and Korbinian P{\"o}ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael K Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ARAxPPIAhq}
}
@inproceedings{
mamba2,
title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
author={Tri Dao and Albert Gu},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ztn8FCR1td}
}
@inproceedings{
meta_la,
title={Meta{LA}: Unified Optimal Linear Approximation to Softmax Attention Map},
author={Yuhong Chou and Man Yao and Kexin Wang and Yuqi Pan and Rui-Jie Zhu and Jibin Wu and Yiran Zhong and Yu Qiao and Bo XU and Guoqi Li},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=Y8YVCOMEpz}
}
@inproceedings{
zhang2024gated,
title={Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
author={Yu Zhang and Songlin Yang and Rui-Jie Zhu and Yue Zhang and Leyang Cui and Yiqiao Wang and Bolun Wang and Freda Shi and Bailin Wang and Wei Bi and Peng Zhou and Guohong Fu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=jY4PhQibmg}
}
@misc{poli_mechanistic_2024,
 author = {Poli, Michael and Thomas, Armin W. and Nguyen, Eric and Ponnusamy, Pragaash and Deiseroth, Björn and Kersting, Kristian and Suzuki, Taiji and Hie, Brian and Ermon, Stefano and Ré, Christopher and Zhang, Ce and Massaroli, Stefano},
 journal = {ArXiv preprint},
 title = {Mechanistic {Design} and {Scaling} of {Hybrid} {Architectures}},
 url = {https://arxiv.org/abs/2403.17844},
 volume = {abs/2403.17844},
 year = {2024}
}

@misc{yang2024gateddeltanetworksimproving,
      title={Gated Delta Networks: Improving Mamba2 with Delta Rule}, 
      author={Songlin Yang and Jan Kautz and Ali Hatamizadeh},
      year={2024},
      eprint={2412.06464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06464}, 
}
@inproceedings{Grazzi2024UnlockingSI,
  title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues},
  author={Riccardo Grazzi and Julien N. Siems and Jorg K. H. Franke and Arber Zela and Frank Hutter and Massimiliano Pontil},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274141450}
}
@inproceedings{bischof_wy_1985,
 author = {Bischof, Christian H. and Loan, Charles Van},
 booktitle = {{SIAM} {Conference} on {Parallel} {Processing} for {Scientific} {Computing}},
 title = {The {WY} representation for products of householder matrices},
 url = {https://api.semanticscholar.org/CorpusID:36094006},
 year = {1985}
}

@article{Merrill2024TheIO,
  title={The Illusion of State in State-Space Models},
  author={William Merrill and Jackson Petty and Ashish Sabharwal},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.08819},
  url={https://api.semanticscholar.org/CorpusID:269149086}
}

@inproceedings{Schlag2021LinearTA,
  title={Linear Transformers Are Secretly Fast Weight Programmers},
  author={Imanol Schlag and Kazuki Irie and J{\"u}rgen Schmidhuber},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235377069}
}

@inproceedings{parallel-martin,
  author       = {Eric Martin and
                  Chris Cundy},
  title        = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
}

@inproceedings{mao-2022-fine,
    title = "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
    author = "Mao, Huanru Henry",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.emnlp-main.697",
    pages = "10236--10242",
}

@article{unreasonable-forget-gate,
  author       = {Jos van der Westhuizen and
                  Joan Lasenby},
  title        = {The unreasonable effectiveness of the forget gate},
  journal      = {CoRR},
  volume       = {abs/1804.04849},
  year         = {2018},
  eprinttype    = {arXiv},
  eprint       = {1804.04849},
}
@inproceedings{linear-xmr-fastweight,
  author       = {Imanol Schlag and
                  Kazuki Irie and
                  J{\"{u}}rgen Schmidhuber},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Linear Transformers Are Secretly Fast Weight Programmers},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {9355--9366},
  publisher    = {{PMLR}},
  year         = {2021},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
}

@article{irie2021going,
  title={Going beyond linear transformers with recurrent fast weight programmers},
  author={Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7703--7717},
  year={2021}
}

@inproceedings{hinton1987using,
  title={Using fast weights to deblur old memories},
  author={Hinton, Geoffrey E and Plaut, David C},
  booktitle={Proceedings of the ninth annual conference of the Cognitive Science Society},
  pages={177--186},
  year={1987}
}
@article{schmidhuber1992learning,
  title={Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={4},
  number={1},
  pages={131--139},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{ba2016using,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@inproceedings{Heinsen2023EfficientPO,
  title={Efficient Parallelization of an Ubiquitous Sequential Computation},
  author={Franz A. Heinsen},
  year={2023},
}
@article{gatedloop,
  title={GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
  author={Tobias Katsch},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.01927},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{qin2022devil,
  title={The devil in linear transformer},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
  journal={arXiv preprint arXiv:2210.10340},
  year={2022}
}

@inproceedings{kasai-etal-2021-finetuning,
    title = "Finetuning Pretrained Transformers into {RNN}s",
    author = "Kasai, Jungo  and
      Peng, Hao  and
      Zhang, Yizhe  and
      Yogatama, Dani  and
      Ilharco, Gabriel  and
      Pappas, Nikolaos  and
      Mao, Yi  and
      Chen, Weizhu  and
      Smith, Noah A.",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.emnlp-main.830",
    pages = "10630--10643",
}

@article{peng2021random,
  title={Random feature attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2103.02143},
  year={2021}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{triton,
  author       = {Philippe Tillet and
                  Hsiang{-}Tsung Kung and
                  David D. Cox},
  editor       = {Tim Mattson and
                  Abdullah Muzahid and
                  Armando Solar{-}Lezama},
  title        = {Triton: an intermediate language and compiler for tiled neural network
                  computations},
  booktitle    = {Proceedings of the 3rd {ACM} {SIGPLAN} International Workshop on Machine
                  Learning and Programming Languages, MAPL@PLDI 2019, Phoenix, AZ, USA,
                  June 22, 2019},
  pages        = {10--19},
  publisher    = {{ACM}},
  year         = {2019},
  doi          = {10.1145/3315508.3329973},
}

@article{rmsnorm,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{arc-ce,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}


@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{reddy2019coqa,
  title={Coqa: A conversational question answering challenge},
  author={Reddy, Siva and Chen, Danqi and Manning, Christopher D},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={249--266},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@Article{SciQA2023,
  author={Auer, S{\"o}ren
  and Barone, Dante A. C.
  and Bartz, Cassiano
  and Cortes, Eduardo G.
  and Jaradeh, Mohamad Yaser
  and Karras, Oliver
  and Koubarakis, Manolis
  and Mouromtsev, Dmitry
  and Pliukhin, Dmitrii
  and Radyush, Daniil
  and Shilin, Ivan
  and Stocker, Markus
  and Tsalapati, Eleni},
  title={The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge},
  journal={Scientific Reports},
  year={2023},
  month={May},
  day={04},
  volume={13},
  number={1},
  pages={7240},
  issn={2045-2322},
  doi={10.1038/s41598-023-33607-z},
}


@article{openbookqa,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}


@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}
@inproceedings{xpos,
  author       = {Yutao Sun and
                  Li Dong and
                  Barun Patra and
                  Shuming Ma and
                  Shaohan Huang and
                  Alon Benhaim and
                  Vishrav Chaudhary and
                  Xia Song and
                  Furu Wei},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {A Length-Extrapolatable Transformer},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {14590--14604},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  doi          = {10.18653/V1/2023.ACL-LONG.816},
}
@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
}


@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}


@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles},
  publisher = {Manifest AI},
  title = {Linear {Transformers} {Are} {Faster} {After} {All}},
  date = {2024-01-05},
  langid = {en},
year={2024},
}


@misc{cerebras2023slimpajama,
 author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
 title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
 year = {2023}
}

@article{jiang2023mistral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
 journal = {ArXiv preprint},
 title = {Mistral 7B},
 volume = {abs/2310.06825},
 year = {2023}
}

@article{pg19,
  author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and
            Hillier, Chloe and Lillicrap, Timothy P},
  title = {Compressive Transformers for Long-Range Sequence Modelling},
  journal = {arXiv preprint},
  year = {2019},
}

@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{fire2024,
  title={Functional interpolation for relative positions improves long context transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}


@article{alibi2021,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{xpos2022,
  title={A length-extrapolatable transformer},
  author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10554},
  year={2022}
}
